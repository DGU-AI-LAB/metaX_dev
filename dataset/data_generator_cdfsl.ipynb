{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from abc import ABC, abstractmethod\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Database(ABC):\n",
    "    def __init__(self, \n",
    "                 raw_database_address, \n",
    "                 database_address, \n",
    "                 random_seed=-1, \n",
    "    ):\n",
    "        if random_seed != -1:\n",
    "            random.seed(random_seed)\n",
    "            tf.random.set_seed(random_seed)\n",
    "\n",
    "        self.raw_database_address = raw_database_address\n",
    "        self.database_address = database_address\n",
    "\n",
    "        self.input_shape = self.get_input_shape()\n",
    "        self.train_folders, self.val_folders, self.test_folders = self.get_train_val_test_folders()\n",
    "        \n",
    "        self.train_folders = self.convert_to_dict(self.train_folders)\n",
    "        self.val_folders = self.convert_to_dict(self.val_folders)\n",
    "        self.test_folders = self.convert_to_dict(self.test_folders)\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_train_val_test_folders(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def preview_image(self, image_path):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_input_shape(self):\n",
    "        pass\n",
    "\n",
    "    def convert_to_dict(self, folders):\n",
    "        if type(folders) == list:\n",
    "            classes = dict()\n",
    "            for folder in folders:\n",
    "                instances = [os.path.join(folder, file_name) for file_name in os.listdir(folder)]\n",
    "                classes[folder] = instances\n",
    "\n",
    "            folders = classes\n",
    "        return folders\n",
    "\n",
    "    def _get_parse_function(self):\n",
    "        def parse_function(example_address):\n",
    "            return example_address\n",
    "        \n",
    "        return parse_function\n",
    "    \n",
    "    def make_labels_dataset(self, n, k, meta_batch_size, steps_per_epoch, one_hot_labels):\n",
    "        labels_dataset = tf.data.Dataset.range(n)\n",
    "        \n",
    "        if one_hot_labels:\n",
    "            labels_dataset = labels_dataset.map(lambda example: tf.one_hot(example, depth=n))\n",
    "\n",
    "        labels_dataset = labels_dataset.interleave(\n",
    "            lambda x: tf.data.Dataset.from_tensors(x).repeat(2 * k),\n",
    "            cycle_length=n,\n",
    "            block_length=k\n",
    "        )\n",
    "        labels_dataset = labels_dataset.repeat(meta_batch_size)\n",
    "        labels_dataset = labels_dataset.repeat(steps_per_epoch)\n",
    "        \n",
    "        return labels_dataset\n",
    "   \n",
    "    def _get_instances(self, k):\n",
    "        def get_instances(class_dir_address):\n",
    "            return tf.data.Dataset.list_files(class_dir_address, shuffle=True).take(2 * k)\n",
    "        \n",
    "        return get_instances\n",
    "    \n",
    "    def keep_keys_with_greater_than_equal_k_items(self, folders_dict, k):\n",
    "        to_be_removed = list()\n",
    "        for folder in folders_dict.keys():\n",
    "            if len(folders_dict[folder]) < k:\n",
    "                to_be_removed.append(folder)\n",
    "\n",
    "        for folder in to_be_removed:\n",
    "            del folders_dict[folder]\n",
    "    \n",
    "    def get_dataset(\n",
    "        self, \n",
    "        folders,\n",
    "        n, \n",
    "        k, \n",
    "        meta_batch_size,\n",
    "        one_hot_labels=True, \n",
    "        reshuffle_each_iteration=True,\n",
    "        random_seed=-1,\n",
    "        dtype=tf.float32,\n",
    "    ):\n",
    "        \n",
    "        def convert_folders_to_list(folders):\n",
    "            if type(folders) == list:\n",
    "                classes = dict()\n",
    "                for folder in folders:\n",
    "                    instances = [os.path.join(folder, file_name) for file_name in os.listdir(folder)]\n",
    "                    classes[folder] = instances\n",
    "                folders = classes\n",
    "                \n",
    "            return folders\n",
    "\n",
    "        folders = convert_folders_to_list(folders)\n",
    "        self.keep_keys_with_greater_than_equal_k_items(folders, k)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(sorted(list(folders.keys())))\n",
    "        steps_per_epoch = len(folders.keys()) // (n * meta_batch_size)\n",
    "        \n",
    "        if random_seed != -1:\n",
    "            dataset = dataset.shuffle(\n",
    "                buffer_size=len(folders.keys()),\n",
    "                reshuffle_each_iteration=reshuffle_each_iteration,\n",
    "                random_seed=random_seed\n",
    "            )\n",
    "            dataset = dataset.interleave(\n",
    "                self._get_instances(k),\n",
    "                cycle_length=n,\n",
    "                block_length=k,\n",
    "                num_parallel_calls=1\n",
    "            )\n",
    "        else:\n",
    "            dataset = dataset.shuffle(\n",
    "                buffer_size=len(folders.keys()),\n",
    "                reshuffle_each_iteration=reshuffle_each_iteration\n",
    "            )\n",
    "            dataset = dataset.interleave(\n",
    "                self._get_instances(k),\n",
    "                cycle_length=n,\n",
    "                block_length=k,\n",
    "                num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "            )\n",
    "    \n",
    "        dataset = dataset.map(self._get_parse_function(), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        labels_dataset = self.make_labels_dataset(n, k, meta_batch_size, steps_per_epoch, one_hot_labels)\n",
    "        \n",
    "        dataset = tf.data.Dataset.zip((dataset, labels_dataset))\n",
    "        dataset = dataset.batch(k, drop_remainder=False)\n",
    "        dataset = dataset.batch(n, drop_remainder=True)\n",
    "        dataset = dataset.batch(2, drop_remainder=True)\n",
    "        dataset = dataset.batch(meta_batch_size, drop_remainder=True)\n",
    "\n",
    "        setattr(dataset, 'steps_per_epoch', steps_per_epoch)\n",
    "        \n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropDiseaseDatabase(Database):\n",
    "    def __init__(self, raw_database_address, random_seed=-1):\n",
    "        super(CropDiseaseDatabase, self).__init__(\n",
    "            raw_database_address,\n",
    "            os.getcwd() + '/dataset/data/CropDiseases',\n",
    "            random_seed=random_seed\n",
    "        )\n",
    "\n",
    "    def get_train_val_test_folders(self):\n",
    "        dataset_folders = list()\n",
    "        for dataset_type in ('train', 'test'):\n",
    "            dataset_base_address = os.path.join(self.database_address, dataset_type)\n",
    "            folders = [\n",
    "                os.path.join(dataset_base_address, class_name) for class_name in os.listdir(dataset_base_address)\n",
    "                ]\n",
    "            dataset_folders.append(folders)\n",
    "\n",
    "        return dataset_folders[0], dataset_folders[1], dataset_folders[1]\n",
    "    \n",
    "    def _get_parse_function(self):\n",
    "        def parse_function(example_address):\n",
    "            image = tf.image.decode_jpeg(tf.io.read_file(example_address), channels=3)\n",
    "            image = tf.image.resize(image, (84, 84))\n",
    "            image = tf.cast(image, tf.float32)\n",
    "\n",
    "            return image / 255.\n",
    "\n",
    "        return parse_function\n",
    "        \n",
    "    def get_input_shape(self):\n",
    "        return 84, 84, 3\n",
    "\n",
    "    def preview_image(self, image_path):\n",
    "        image = Image.open(image_path)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "cropDisease = CropDiseaseDatabase(raw_database_address='\\dataset\\data\\CropDiseases', random_seed = -1)\n",
    "crop_folders = cropDisease.get_train_val_test_folders()\n",
    "print(len(crop_folders[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((64, 2, 5, None, 84, 84, 3), (64, 2, 5, None, 5)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "crop_dataset = cropDisease.get_dataset(crop_folders[0], 5, 1, 64)\n",
    "print(crop_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuroSatDatabase(Database):\n",
    "    def __init__(self, raw_data_address, random_seed=-1):\n",
    "        super(EuroSatDatabase, self).__init__(\n",
    "            raw_data_address,\n",
    "            os.getcwd() + '/dataset/data/EuroSAT',\n",
    "            random_seed=random_seed\n",
    "        )\n",
    "\n",
    "    def get_train_val_test_folders(self):\n",
    "        base = os.path.join(self.database_address, '2750')\n",
    "        folders = [os.path.join(base, folder_name) for folder_name in os.listdir(base)]\n",
    "\n",
    "        return folders, folders, folders\n",
    "\n",
    "    def _get_parse_function(self):\n",
    "        def parse_function(example_address):\n",
    "            image = tf.image.decode_jpeg(tf.io.read_file(example_address), channels=3)\n",
    "            image = tf.image.resize(image, (84, 84))\n",
    "            image = tf.cast(image, tf.float32)\n",
    "\n",
    "            return image / 255.\n",
    "\n",
    "        return parse_function\n",
    "\n",
    "    def get_input_shape(self):\n",
    "        return 84, 84, 3\n",
    "\n",
    "    def preview_image(self, image_path):\n",
    "        image = Image.open(image_path)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "euroSatDatabase = EuroSatDatabase('', random_seed = -1)\n",
    "euro_classes = euroSatDatabase.get_train_val_test_folders()\n",
    "print(len(euro_classes[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((64, 2, 5, None, 84, 84, 3), (64, 2, 5, None, 5)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "euro_dataset = euroSatDatabase.get_dataset(euro_classes[0], 5, 5, 64)\n",
    "print(euro_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISICDatabase(Database):\n",
    "    def __init__(self, raw_data_address, random_seed=-1):\n",
    "        super(ISICDatabase, self).__init__(\n",
    "            raw_data_address,\n",
    "            os.getcwd() + '/dataset/data/ISIC',\n",
    "            random_seed=random_seed\n",
    "        )\n",
    "\n",
    "    def get_train_val_test_folders(self):\n",
    "        gt_file = os.path.join(\n",
    "            self.database_address,\n",
    "            'ISIC2018_Task3_Training_GroundTruth',\n",
    "            'ISIC2018_Task3_Training_GroundTruth.csv'\n",
    "        )\n",
    "        content = pd.read_csv(gt_file)\n",
    "        class_names = list(content.columns[1:])\n",
    "\n",
    "        images = list(content.iloc[:, 0])\n",
    "\n",
    "        labels = np.array(content.iloc[:, 1:])\n",
    "        labels = np.argmax(labels, axis=1)\n",
    "\n",
    "        classes = dict()\n",
    "        for class_name in class_names:\n",
    "            classes[class_name] = list()\n",
    "\n",
    "        for image, label in zip(images, labels):\n",
    "            classes[class_names[label]].append(\n",
    "                os.path.join(self.database_address, 'ISIC2018_Task3_Training_Input', image + '.jpg')\n",
    "            )\n",
    "\n",
    "        return classes, classes, classes\n",
    "\n",
    "\n",
    "    def _get_parse_function(self):\n",
    "        def parse_function(example_address):\n",
    "            image = tf.image.decode_jpeg(tf.io.read_file(example_address), channels=3)\n",
    "            image = tf.image.resize(image, (84, 84))\n",
    "            image = tf.cast(image, tf.float32)\n",
    "\n",
    "            return image / 255.\n",
    "\n",
    "        return parse_function\n",
    "\n",
    "    def get_input_shape(self):\n",
    "        return 84, 84, 3\n",
    "\n",
    "    def preview_image(self, image_path):\n",
    "        image = Image.open(image_path)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "isicDatabase = ISICDatabase('', random_seed=-1)\n",
    "isic_folders = isicDatabase.get_train_val_test_folders()\n",
    "print(len(isic_folders[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((64, 2, 5, None, 84, 84, 3), (64, 2, 5, None, 5)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "isic_dataset = isicDatabase.get_dataset(isic_folders[0], 5, 5, 64)\n",
    "print(isic_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXRay8Database(Database):\n",
    "    def __init__(self, raw_data_address, random_seed=-1):\n",
    "        super(ChestXRay8Database, self).__init__(\n",
    "            raw_data_address, \n",
    "            os.getcwd() + '/dataset/data/chestX',\n",
    "            random_seed=random_seed\n",
    "        )\n",
    "\n",
    "    def get_train_val_test_folders(self):\n",
    "        image_paths = dict()\n",
    "\n",
    "        for folder_name in os.listdir(self.database_address):\n",
    "            if os.path.isdir(os.path.join(self.database_address, folder_name)):\n",
    "                base_address = os.path.join(self.database_address, folder_name)\n",
    "                for item in os.listdir(os.path.join(base_address, 'images')):\n",
    "                    image_paths[item] = os.path.join(base_address, 'images', item)\n",
    "\n",
    "        gt_file = os.path.join(self.database_address, 'Data_Entry_2017.csv')\n",
    "        class_names = [\n",
    "            \"Atelectasis\", \"Cardiomegaly\", \"Effusion\", \"Infiltration\", \"Mass\", \"Nodule\", \"Pneumonia\", \"Pneumothorax\"\n",
    "        ]\n",
    "\n",
    "        content = pd.read_csv(gt_file)\n",
    "        images = list(content.iloc[:, 0])\n",
    "\n",
    "        labels = np.asarray(content.iloc[:, 1])\n",
    "\n",
    "        classes = dict()\n",
    "        for class_name in class_names:\n",
    "            classes[class_name] = list()\n",
    "\n",
    "        for image, label in zip(images, labels):\n",
    "            label = label.split(\"|\")\n",
    "            if(\n",
    "                len(label) == 1 and \n",
    "                label[0] != \"No Finding\" and\n",
    "                label[0] != \"Pneumonia\" and \n",
    "                label[0] in class_names\n",
    "            ):\n",
    "                classes[label[0]].append(image_paths[image])\n",
    "\n",
    "        return classes, classes, classes\n",
    "\n",
    "    def _get_parse_function(self):\n",
    "        def parse_function(example_address):\n",
    "            image = tf.image.decode_png(tf.io.read_file(example_address), channels=3)\n",
    "            image = tf.image.resize(image, self.get_input_shape()[:2])\n",
    "            image = tf.cast(image, tf.float32)\n",
    "            return image / 255.\n",
    "\n",
    "        return parse_function\n",
    "\n",
    "    def get_input_shape(self):\n",
    "        return 84, 84, 3\n",
    "\n",
    "    def preview_image(self, image_path):\n",
    "        image = Image.open(image_path)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "chestDatabase = ChestXRay8Database('', random_seed=-1)\n",
    "chest_folders = chestDatabase.get_train_val_test_folders()\n",
    "print(len(chest_folders[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((64, 2, 5, None, 84, 84, 3), (64, 2, 5, None, 5)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "chest_dataset = chestDatabase.get_dataset(chest_folders[0], 5, 1, 64)\n",
    "print(chest_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
